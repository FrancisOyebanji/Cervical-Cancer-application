{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3159659,"sourceType":"datasetVersion","datasetId":1921923},{"sourceId":3189191,"sourceType":"datasetVersion","datasetId":1936552},{"sourceId":3329902,"sourceType":"datasetVersion","datasetId":2011775}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-14T02:14:49.254163Z","iopub.execute_input":"2024-06-14T02:14:49.254587Z","iopub.status.idle":"2024-06-14T02:14:50.382209Z","shell.execute_reply.started":"2024-06-14T02:14:49.254545Z","shell.execute_reply":"2024-06-14T02:14:50.380888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport keras\nimport matplotlib\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pylab as pl\nimport sklearn\nimport missingno as msno\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom collections import Counter\n\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n\n\nimport imblearn\nimport collections\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.combine import SMOTEENN\nfrom collections import Counter\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SVMSMOTE\nfrom numpy import where\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nimport joblib\nfrom joblib import dump, load\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:01.087845Z","iopub.execute_input":"2024-06-14T02:15:01.088256Z","iopub.status.idle":"2024-06-14T02:15:16.675373Z","shell.execute_reply.started":"2024-06-14T02:15:01.088215Z","shell.execute_reply":"2024-06-14T02:15:16.674446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check package versions\n\nprint('numpy --',np.__version__)\nprint('pandas --',pd.__version__)\nprint('keras --',keras.__version__)\nprint('matplotlib --',matplotlib.__version__)\nprint('imblearn --',imblearn.__version__)\nprint('seaborn --',sns.__version__)\nprint('sklearn --',sklearn.__version__)\nprint('joblib --',joblib.__version__)\n\n# numpy -- 1.20.3\n# pandas -- 1.3.5\n# keras -- 2.6.0\n# matplotlib -- 3.5.1\n# imblearn -- 0.9.0\n# seaborn -- 0.11.2\n# sklearn -- 0.23.2\n# joblib -- 1.1.0","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:24.737934Z","iopub.execute_input":"2024-06-14T02:15:24.738945Z","iopub.status.idle":"2024-06-14T02:15:24.747444Z","shell.execute_reply.started":"2024-06-14T02:15:24.738909Z","shell.execute_reply":"2024-06-14T02:15:24.746177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# power = PowerTransformer(method='yeo-johnson')# try 'yeo-johnson' or 'box-cox'\n# #df_scaled = pd.DataFrame(power.fit_transform(x), columns = x.columns)\n# testing = power.fit_transform([3,6,8,1,4,9,1])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:38.983415Z","iopub.execute_input":"2023-03-15T00:11:38.983831Z","iopub.status.idle":"2023-03-15T00:11:39.019043Z","shell.execute_reply.started":"2023-03-15T00:11:38.983790Z","shell.execute_reply":"2023-03-15T00:11:39.017580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import PowerTransformer\n# data = [[3, 0], [33, 0], [3, 1], [3, 1]]\n# scaler = StandardScaler()\n# power = PowerTransformer(method='yeo-johnson')\n# print(scaler.fit(data))\n# print(power.fit(data))\n# StandardScaler()\n# #print(scaler.mean_)\n# print(scaler.transform(data))\n# print(power.transform(data))\n\n# print(scaler.transform([[2, 2]]))\n# print(power.transform([[2, 2]]))\n# data = pd.DataFrame(data)\n# data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:39.020622Z","iopub.execute_input":"2023-03-15T00:11:39.022039Z","iopub.status.idle":"2023-03-15T00:11:39.034328Z","shell.execute_reply.started":"2023-03-15T00:11:39.021976Z","shell.execute_reply":"2023-03-15T00:11:39.033082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"\ndf=pd.read_csv('../input/d/datasets/oyeb0000/cancer-dataset/imputed_initial.csv')  # Imputed dataset\n#df=pd.read_csv('../input/raw-cancer-data/risk.csv')   # Raw dataset)\n\n#df.head(11)\n#df.iloc[2]","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:32.635614Z","iopub.execute_input":"2024-06-14T02:15:32.636691Z","iopub.status.idle":"2024-06-14T02:15:32.661708Z","shell.execute_reply.started":"2024-06-14T02:15:32.636637Z","shell.execute_reply":"2024-06-14T02:15:32.660681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.drop(['STDs: Time since last diagnosis','STDs: Time since first diagnosis'],axis=1,inplace=True)\ndf.drop(['STDs:cervical condylomatosis'],axis=1,inplace=True) # Column informationt is uniform, has one value type\ndf = df.replace('?', np.NaN)\ndf = df.replace('NA', np.NaN)\ndf.dtypes\n\n\n# df[pd.isnull(df.Age)]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:35.427844Z","iopub.execute_input":"2024-06-14T02:15:35.428269Z","iopub.status.idle":"2024-06-14T02:15:35.457156Z","shell.execute_reply.started":"2024-06-14T02:15:35.428237Z","shell.execute_reply":"2024-06-14T02:15:35.455767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check data completeness","metadata":{}},{"cell_type":"code","source":"#msno.matrix(df, fontsize=12, figsize=(15,5),labels=True)# filter=None, n=0, p=0, sort=None, figsize=(25, 10), width_ratios=(15, 1), color=(0.25, 0.25, 0.25), fontsize=16, labels=None, sparkline=True, inline=False, freq=None, ax=None)\n#plt.savefig(\"matrix.jpg\",format='jpg',dpi=100)\nmsno.bar(df,fontsize=12, figsize=(15,5),labels=True)\nplt.savefig(\"bar_for complete data.jpg\",format='jpg',dpi=100)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:41.211448Z","iopub.execute_input":"2024-06-14T02:15:41.212501Z","iopub.status.idle":"2024-06-14T02:15:44.296837Z","shell.execute_reply.started":"2024-06-14T02:15:41.212448Z","shell.execute_reply":"2024-06-14T02:15:44.295647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## check data distribution","metadata":{}},{"cell_type":"code","source":"#sns.set(font_scale=1.3)\nsns.set(rc={'figure.figsize':(12,8),'axes.facecolor':'white', 'figure.facecolor':'white',\"figure.dpi\":100, 'savefig.dpi':300},font_scale = 1.50)\nsns.set_style(\"ticks\")\nplt.figure(figsize=(15, 6))\nsns.countplot(x=df['Age'],hue=df['Hinselmann'])# Schiller  Biopsy\n\nplt.figure(figsize=(15, 6))\nsns.countplot(x=df['First sexual intercourse'],hue=df['Biopsy'])\n\nplt.figure(figsize=(15, 6))\nsns.countplot(x=df['STDs (number)'],hue=df['Biopsy'])\n\nplt.figure(figsize=(15, 6))\nsns.countplot(x=df['STDs: Number of diagnosis'],hue=df['Biopsy'])\n\n\nplt.figure(figsize=(15, 6),frameon=False)\nsns.countplot(x=df['Dx:CIN'],hue=df['Citology'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:15:52.908435Z","iopub.execute_input":"2024-06-14T02:15:52.909396Z","iopub.status.idle":"2024-06-14T02:15:55.122760Z","shell.execute_reply.started":"2024-06-14T02:15:52.909361Z","shell.execute_reply":"2024-06-14T02:15:55.121275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Column Data Distribution","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8),'axes.facecolor':'white', 'figure.facecolor':'white',\"figure.dpi\":100, 'savefig.dpi':300},font_scale = .50)\nsns.set_style(\"ticks\")\n\ndf.hist(grid=False, bins=30)\n\n#Xi.\nplt.subplots_adjust(hspace = 0.7,wspace=0.1940148)\n\n#plt.hist(datasets['Target'])\nplt.xlabel('Age of Players')\nplt.ylabel('# of Players')\nplt.title('Age Distribution')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T02:16:02.966371Z","iopub.execute_input":"2024-06-14T02:16:02.966938Z","iopub.status.idle":"2024-06-14T02:16:10.799725Z","shell.execute_reply.started":"2024-06-14T02:16:02.966903Z","shell.execute_reply":"2024-06-14T02:16:10.798647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove \"outliers\" #JS \ndf.drop(df.index[df['Age'] > 55], inplace = True)\n#df.drop(df.index[df['Number of sexual partners'] > 8], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.270717Z","iopub.execute_input":"2023-03-15T00:11:48.271021Z","iopub.status.idle":"2023-03-15T00:11:48.278782Z","shell.execute_reply.started":"2023-03-15T00:11:48.270990Z","shell.execute_reply":"2023-03-15T00:11:48.277261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\n\ndf = df.replace('?', np.NaN)\n\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.280548Z","iopub.execute_input":"2023-03-15T00:11:48.281419Z","iopub.status.idle":"2023-03-15T00:11:48.298886Z","shell.execute_reply.started":"2023-03-15T00:11:48.281379Z","shell.execute_reply":"2023-03-15T00:11:48.297609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.300642Z","iopub.execute_input":"2023-03-15T00:11:48.301307Z","iopub.status.idle":"2023-03-15T00:11:48.311161Z","shell.execute_reply.started":"2023-03-15T00:11:48.301270Z","shell.execute_reply":"2023-03-15T00:11:48.309802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop_duplicates()\ndf_num=df.select_dtypes(include=\"number\")\ndf_num.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.313001Z","iopub.execute_input":"2023-03-15T00:11:48.313302Z","iopub.status.idle":"2023-03-15T00:11:48.336486Z","shell.execute_reply.started":"2023-03-15T00:11:48.313272Z","shell.execute_reply":"2023-03-15T00:11:48.335714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat=df.select_dtypes(include=\"object\")\ndf_cat1=df_cat.apply(pd.to_numeric)\ndf_cat1.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.337486Z","iopub.execute_input":"2023-03-15T00:11:48.338425Z","iopub.status.idle":"2023-03-15T00:11:48.349586Z","shell.execute_reply.started":"2023-03-15T00:11:48.338391Z","shell.execute_reply":"2023-03-15T00:11:48.347581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([df_cat1,df_num],axis=1,join=\"inner\")","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.351035Z","iopub.execute_input":"2023-03-15T00:11:48.351389Z","iopub.status.idle":"2023-03-15T00:11:48.360269Z","shell.execute_reply.started":"2023-03-15T00:11:48.351355Z","shell.execute_reply":"2023-03-15T00:11:48.358859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#impute missing values\nfrom sklearn.impute import SimpleImputer\nimpute = SimpleImputer()\ndf_clean = pd.DataFrame(impute.fit_transform(df))\ndf_clean.columns=df.columns\ndf_clean.describe()\ndf_copy = df_clean","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.363170Z","iopub.execute_input":"2023-03-15T00:11:48.363822Z","iopub.status.idle":"2023-03-15T00:11:48.444238Z","shell.execute_reply.started":"2023-03-15T00:11:48.363760Z","shell.execute_reply":"2023-03-15T00:11:48.443101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BoxPlot showing raw distribution of features\n\nplt.figure(figsize=(13,10))\ndf_clean.iloc[:,:].boxplot()\nplt.title('Percentile Distribution of Features', fontsize=17)\n# Get current axes\nax = plt.gca()\n# Make space for and rotate the x-axis tick labels\nplt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:48.445638Z","iopub.execute_input":"2023-03-15T00:11:48.446003Z","iopub.status.idle":"2023-03-15T00:11:49.002765Z","shell.execute_reply.started":"2023-03-15T00:11:48.445969Z","shell.execute_reply":"2023-03-15T00:11:49.001410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert age to Age groups and First sexual intercourse to a group\n\nage_bins = np.arange(9, 55 , 5)#\n#age_bins = np.arange(13, 69 , 6)\nage_labels = np.arange(1, 10)#\n#age_labels = np.arange(1, 10)\n\ndf_clean['age_group'] = pd.cut(df_clean.Age, bins = age_bins, labels = age_labels)\ndf_clean['age_group'] =  pd.to_numeric(df_clean['age_group'])\n\n# df[['Age','age_group']].head(20\n# Similarly convert first sexual intercourse age, Min age: 10, Max age: 32\nfsi_bins = np.arange(9, 35 , 5)\nfsi_labels = np.arange(1, 6)\ndf_clean['fsi_group'] = pd.cut(df_clean['First sexual intercourse'], bins = fsi_bins, labels = fsi_labels)\ndf_clean['fsi_group'] =  pd.to_numeric(df_clean['fsi_group'])\n\n# Drop Age, First Sexual Intercourse columns\ndf_clean = df_clean.drop(columns=['Age', 'First sexual intercourse'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:49.004148Z","iopub.execute_input":"2023-03-15T00:11:49.004483Z","iopub.status.idle":"2023-03-15T00:11:49.020973Z","shell.execute_reply.started":"2023-03-15T00:11:49.004452Z","shell.execute_reply":"2023-03-15T00:11:49.019779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for Original features\n\nnumerical=['age_group','Number of sexual partners','fsi_group','Num of pregnancies','Smokes (years)','Smokes (packs/year)','Hormonal Contraceptives (years)'] # --> Choosing the proper numerical features \n\n#numerical=['Age','Number of sexual partners','First sexual intercourse','Num of pregnancies','Hormonal Contraceptives (years)'] # --> Choosing the proper numerical features \n#numerical=['Number of sexual partners','Num of pregnancies','Hormonal Contraceptives (years)'] \n\n\n\ndf_copy = df_clean.copy()\ndf_copy[numerical]=df_copy[numerical].astype('float64')\ndf_copy[numerical].plot(kind='box',subplots=True, layout=(4,4), fontsize=8, figsize=(14,14))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:49.022981Z","iopub.execute_input":"2023-03-15T00:11:49.024014Z","iopub.status.idle":"2023-03-15T00:11:49.922847Z","shell.execute_reply.started":"2023-03-15T00:11:49.023944Z","shell.execute_reply":"2023-03-15T00:11:49.921742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analytics (Correlation & Feature Interaction)\n- Rank feature importance\n- Remove one of highly correlated pair\n- identify pair for feature interaction\n- create new feature \n","metadata":{}},{"cell_type":"code","source":"# Correlation\n\ntargets=['Biopsy','Citology','Hinselmann','Schiller']\ndf_targets = df_copy[targets].astype('float64')\n\n# plot the heat map for the numeric dataset\n\nheatmap = sns.heatmap(df_copy[numerical].corr())\n#heatmap = sns.heatmap(df_copy[numerical].corr(),mask=mask,vmin=-1,vmax=1,annot=True,cmap='BrBG')\n#heatmap.set_title('Numeric dataset Correlation Heatmap',fontdict={'fontsize':18})#, pad=16);","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:49.924191Z","iopub.execute_input":"2023-03-15T00:11:49.924503Z","iopub.status.idle":"2023-03-15T00:11:50.210763Z","shell.execute_reply.started":"2023-03-15T00:11:49.924473Z","shell.execute_reply":"2023-03-15T00:11:50.209576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style = 'ticks') # Set the background to dark\nsns.pairplot(df_copy, vars=['Biopsy','Citology','Hinselmann','Schiller'] ) # Create a matrix scatterplot","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:50.212537Z","iopub.execute_input":"2023-03-15T00:11:50.212866Z","iopub.status.idle":"2023-03-15T00:11:53.180327Z","shell.execute_reply.started":"2023-03-15T00:11:50.212835Z","shell.execute_reply":"2023-03-15T00:11:53.178473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create new features\n\n#df_copy = pd.concat(df_copy, ignore_index = True)\n\ndf_copy['new_feature'] = df_copy['Number of sexual partners']*(df_copy['fsi_group'])\ndf_copy['new_feature2'] = df_copy['Num of pregnancies']/(df_copy['age_group'])*2\nstat = df_copy.describe()\nstat.to_csv('data_statistics.csv')\n\nsns.catplot(y=\"age_group\", hue=\"Citology\", kind=\"count\", palette=None, edgecolor=\".16\",data=df_copy)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:53.182481Z","iopub.execute_input":"2023-03-15T00:11:53.182911Z","iopub.status.idle":"2023-03-15T00:11:53.601026Z","shell.execute_reply.started":"2023-03-15T00:11:53.182875Z","shell.execute_reply":"2023-03-15T00:11:53.599008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create new feature\n\n# #df_copy[numerical][\"new_feature\"] = df_copy[numerical][\"Age\"] * df_copy[numerical][\"Number of sexual partners\"]#\n# #df_copy[numerical][\"new_feature2\"] = df_copy[numerical][\"Hormonal Contraceptives (years)\"] * df_copy[numerical][\"First sexual intercourse\"]#\n\n# class CustomTransformer(TransformerMixin):\n#     def fit(self, X, y=None):\n#         return self\n\n#     def transform(self, X, y=None):\n#         # Copy X to avoid mutating the original dataset\n#         X_ = X.copy()\n#         # change new_feature and right member according to your needs\n#         #X_[\"new_feature\"] = X_[\"Number of sexual partners\"] * X_[\"First sexual intercourse\"]#\n#         X_[\"new_feature\"] = X_[\"Number of sexual partners\"] * X_[\"fsi_group\"] # for group dataset\n#         return X_\n\n\n# # Instantiate DataSet, your Transformer and Pipeline\n# X = df_copy[['Number of sexual partners','fsi_group']] # for group dataset\n# #X = df_copy[['Number of sexual partners','First sexual intercourse']] # for normal dataset\n# custom = CustomTransformer()\n# pipeline = Pipeline([(\"\", custom)])\n# original_feature = df_copy\n# # Test it\n# pipeline.fit(X)\n# X = pipeline.transform(X)\n# new_feature = X['new_feature']\n# df_copy = pd.concat([original_feature,new_feature],axis = 1)\n# df_copy.reset_index()\n\n# #numerical=['Age','Number of sexual partners','new_feature','First sexual intercourse','Num of pregnancies',\n# #          'Smokes (years)','Smokes (packs/year)','Hormonal Contraceptives (years)'] # --> Choosing the proper numerical features\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:53.602262Z","iopub.execute_input":"2023-03-15T00:11:53.602737Z","iopub.status.idle":"2023-03-15T00:11:53.609464Z","shell.execute_reply.started":"2023-03-15T00:11:53.602708Z","shell.execute_reply":"2023-03-15T00:11:53.608077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Manipulation and data splitting","metadata":{}},{"cell_type":"code","source":"numerical=['Number of sexual partners','Num of pregnancies','Hormonal Contraceptives (years)'] # Choosing the proper numerical features \n\n#df_copy = df_clean.copy()\ndf_copy[numerical]=df_copy[numerical].astype('float64')\ndf_copy[numerical].plot(kind='box',subplots=True, layout=(4,4), fontsize=8, figsize=(14,14))\n\ndf_copy[numerical]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:53.611440Z","iopub.execute_input":"2023-03-15T00:11:53.611825Z","iopub.status.idle":"2023-03-15T00:11:54.226613Z","shell.execute_reply.started":"2023-03-15T00:11:53.611796Z","shell.execute_reply":"2023-03-15T00:11:54.224944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform some numeric feature to be between a range between 25th and 75th percentile\n\nIQR=df_copy[numerical].describe().T['75%']-df_copy[numerical].describe().T['25%']\n\nmin,max=[df_copy[numerical].describe().T['25%']-(IQR*1.5),df_copy[numerical].describe().T['75%']+(IQR*1.5)]\n\nfor i in numerical:\n    print('range of',i,'b/w',min[i],'and',max[i])\n\nfor i in numerical:\n    df_copy[i][df_copy[i]>max[i]]=max[i]\n    df_copy[i][df_copy[i]<min[i]]=min[i]\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.228435Z","iopub.execute_input":"2023-03-15T00:11:54.228799Z","iopub.status.idle":"2023-03-15T00:11:54.280865Z","shell.execute_reply.started":"2023-03-15T00:11:54.228764Z","shell.execute_reply":"2023-03-15T00:11:54.279500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy.head(11)\ndf_copy.iloc[2]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.282234Z","iopub.execute_input":"2023-03-15T00:11:54.283014Z","iopub.status.idle":"2023-03-15T00:11:54.292810Z","shell.execute_reply.started":"2023-03-15T00:11:54.282980Z","shell.execute_reply":"2023-03-15T00:11:54.291208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata_statistics = df_copy.describe()\ndf_copy.describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.302415Z","iopub.execute_input":"2023-03-15T00:11:54.302781Z","iopub.status.idle":"2023-03-15T00:11:54.441314Z","shell.execute_reply.started":"2023-03-15T00:11:54.302749Z","shell.execute_reply":"2023-03-15T00:11:54.440049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting, standardizing and normalizing the test and train dataset separately\n\nx = df_copy.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\ny = df_copy['Hinselmann']\n\nSS = StandardScaler()\nMMS = MinMaxScaler()\npower = PowerTransformer(method='yeo-johnson', standardize = False )# try 'yeo-johnson' or 'box-cox'\n\ndf_scaled_1 = pd.DataFrame(SS.fit_transform(x), columns = x.columns)\n#check = df_scaled.iloc[2]\ndf_scaled = pd.DataFrame(power.fit_transform(df_scaled_1), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS\n\n\n\nx_train,x_test,y_train,y_test = train_test_split(df_scaled,y, stratify=y, test_size = 0.2, random_state = 212) # try 0.3, 0.35, 0.4\n# x_train,x_test,y_train,y_test = train_test_split(x,y, stratify=y, test_size = 0.5, random_state = 2) # try 0.3, 0.35, 0.4\n\n# x_train_transformed = pd.DataFrame(SS.fit_transform(x_train), columns = x.columns)\n# x_train = pd.DataFrame(power.fit_transform(x_train_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n\n# x_test_transformed = pd.DataFrame(SS.fit_transform(x_test), columns = x.columns)\n# x_test = pd.DataFrame(power.fit_transform(x_test_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.442500Z","iopub.execute_input":"2023-03-15T00:11:54.442803Z","iopub.status.idle":"2023-03-15T00:11:54.518088Z","shell.execute_reply.started":"2023-03-15T00:11:54.442776Z","shell.execute_reply":"2023-03-15T00:11:54.516992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.519291Z","iopub.execute_input":"2023-03-15T00:11:54.519625Z","iopub.status.idle":"2023-03-15T00:11:54.554444Z","shell.execute_reply.started":"2023-03-15T00:11:54.519595Z","shell.execute_reply":"2023-03-15T00:11:54.553739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport keras\nimport matplotlib\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pylab as pl\nimport sklearn\nimport missingno as msno\nimport datetime\nfrom datetime import datetime\nfrom numpy import argmax\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom collections import Counter\n!pip install outlier_utils\nfrom outliers import smirnov_grubbs as grubbs\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\nfrom sklearn.metrics import roc_curve,f1_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\n\nimport imblearn\nimport collections\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.combine import SMOTEENN\nfrom collections import Counter\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SVMSMOTE\nfrom numpy import where\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nimport joblib\nfrom joblib import dump, load\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:11:54.555469Z","iopub.execute_input":"2023-03-15T00:11:54.556148Z","iopub.status.idle":"2023-03-15T00:12:04.885411Z","shell.execute_reply.started":"2023-03-15T00:11:54.556115Z","shell.execute_reply":"2023-03-15T00:12:04.884037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final model\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nx = df_copy.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\ny = df_copy['Init_assess']\n\nX = pd.DataFrame(SS.fit_transform(x), columns = x.columns)\nX = pd.DataFrame(power.fit_transform(X), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, stratify=y, test_size = 0.2, random_state = 25)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:04.888450Z","iopub.execute_input":"2023-03-15T00:12:04.888860Z","iopub.status.idle":"2023-03-15T00:12:04.963161Z","shell.execute_reply.started":"2023-03-15T00:12:04.888825Z","shell.execute_reply":"2023-03-15T00:12:04.961384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [0.514,0.2525,0.5,0.525]\nfrom imblearn.pipeline import Pipeline\n\n\n#df = X.drop(columns=['3_wk_R','5_day_R', '1_wk_R','DURATION','CUMM'])#,  '1_wk_R','CUMM',\n#X_train, X_test, y_train, y_test = train_test_split(df, y, test_size = 0.2,random_state = 740, stratify  = y)\n\nX_smote = X_train\ny_smote = y_train\ncounter = Counter(y)\nprint(counter)\nover = SMOTE(sampling_strategy=1.0)\n#under = RandomUnderSampler(sampling_strategy = 0.5)\n#steps = [('o', over),('u',under)]\nsteps = [('o', over)]\npipeline = Pipeline(steps = steps)\nx_trains, y_trains = pipeline.fit_resample(X_smote, y_smote)\n# summarize the new class distribution\ncounter = Counter(y_trains)\nprint(counter)\n\n# scores = []\n# model_list = []\n\nmodel_list = list()\nmodel_list.append(('KNN', KNeighborsClassifier(metric= 'cosine', n_neighbors = 2, weights = 'distance').fit(x_trains, y_trains)))\nmodel_list.append(('LR', LogisticRegression(C =  0.01, penalty = 'l2', solver = 'newton-cg').fit(x_trains, y_trains)))\nmodel_list.append(('RF', RandomForestClassifier(max_depth = 15, max_features = 5, min_samples_split = 3, n_estimators = 100).fit(x_trains, y_trains)))\nmodel_list.append(('GBC', GradientBoostingClassifier(learning_rate= 0.1, max_depth= 9, n_estimators= 100, subsample =  0.7).fit(x_trains, y_trains)))\n\n\nensemble = VotingClassifier(estimators = model_list, voting='soft', weights=scores) # weights=scores\nensemble.fit(x_trains, y_trains) # x_train, y_train   df_final_model,y\n# make predictions on test set\ny_pred = ensemble.predict(X_test)\n# evaluate predictions\nscore = recall_score(y_test, y_pred)\nprint('Weighted Avg Recall: %.3f' % (score*100))\n\n# confusion matrix on the test data.\n#print('\\nConfusion matrix of GBC optimized for {} on the test data:'.format(refit_score))\nprint(pd.DataFrame(confusion_matrix(y_test, y_pred),\n             columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:04.965052Z","iopub.execute_input":"2023-03-15T00:12:04.965392Z","iopub.status.idle":"2023-03-15T00:12:06.774099Z","shell.execute_reply.started":"2023-03-15T00:12:04.965360Z","shell.execute_reply":"2023-03-15T00:12:06.771928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = LogisticRegression()#solver='lbfgs'\n#KNN =  KNeighborsClassifier(metric = 'manhattan', n_neighbors= 1, weights= 'uniform').fit(X_train, y_train)\n#model =  KNeighborsClassifier()\nmodel = GradientBoostingClassifier()\n#model = RandomForestClassifier(n_estimators=100, random_state=0)\n\nmodel.fit(x_trains, y_trains)\ny_pred = model.predict(X_test)\n\n# predict probabilities\nyhat = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nyhat = yhat[:, 1]\n# calculate roc curves\nprecision, recall, thresholds = precision_recall_curve(y_test, yhat)\n# convert to f score\nfscore = (2 * precision * recall) / (precision + recall)\n# locate the index of the largest f score\nix = argmax(fscore)\nprint('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n# plot the roc curve for the model\nno_skill = len(y_test[y_test==1]) / len(y_test)#\n\npyplot.plot([0,1], [0.5,0.5], linestyle='--', label='Random Guess')\npyplot.plot(recall, precision, marker='.', label='GBC')\nplt.plot([1,1, 0], [0,1, 1], linestyle='--', lw=2, color='r', label='Perfect model')\n\npyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('Recall')\npyplot.title('PR Curve')\npyplot.ylabel('Precision')\npyplot.legend()\n# show the plot\npyplot.show()\nplt.savefig('PR.jpg', transparent = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:06.775288Z","iopub.execute_input":"2023-03-15T00:12:06.775781Z","iopub.status.idle":"2023-03-15T00:12:07.520515Z","shell.execute_reply.started":"2023-03-15T00:12:06.775750Z","shell.execute_reply":"2023-03-15T00:12:07.519368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_proba_log_test = model.predict_proba(X_test)[::,1]\nfpr_log_test, tpr_log_test, _ = metrics.roc_curve(y_test,  y_pred_proba_log_test)\nauc_log_test = metrics.roc_auc_score(y_test, y_pred_proba_log_test)\n\ny_pred_proba_log_train = model.predict_proba(X_train)[::,1]\nfpr_log_train, tpr_log_train, _ = metrics.roc_curve(y_train,  y_pred_proba_log_train)\nauc_log_train = metrics.roc_auc_score(y_train, y_pred_proba_log_train)\n\n#auc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr_log_test,tpr_log_test,label=\"Testing, AUC=\"+str(auc_log_test))\nplt.plot(fpr_log_train,tpr_log_train,label=\"Training, AUC=\"+str(auc_log_train))\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')\nplt.legend(loc=4)\n# axis labels\npyplot.title('ROC Curve')#\npyplot.ylabel('True positive rate')\npyplot.xlabel('False positive rate')\nplt.show()\nplt.savefig('ROC.jpg', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:07.522095Z","iopub.execute_input":"2023-03-15T00:12:07.522809Z","iopub.status.idle":"2023-03-15T00:12:08.039082Z","shell.execute_reply.started":"2023-03-15T00:12:07.522769Z","shell.execute_reply":"2023-03-15T00:12:08.038060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.savefig('CM.jpg', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:08.040452Z","iopub.execute_input":"2023-03-15T00:12:08.041540Z","iopub.status.idle":"2023-03-15T00:12:08.703291Z","shell.execute_reply.started":"2023-03-15T00:12:08.041473Z","shell.execute_reply":"2023-03-15T00:12:08.702139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nyhat = ensemble.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nyhat = yhat[:, 1]\n# calculate roc curves\nprecision, recall, thresholds = precision_recall_curve(y_test, yhat)\n# convert to f score\nfscore = (2 * precision * recall) / (precision + recall)\n# locate the index of the largest f score\nix = argmax(fscore)\nprint('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n# plot the roc curve for the model\nno_skill = len(y_test[y_test==1]) / len(y_test)\n\npyplot.plot([0,1], [0.5,0.5], linestyle='--', label='Baseline Classifier')\npyplot.plot(recall, precision, marker='.', label='Ensemble')\nplt.plot([1,1, 0], [0,1, 1], linestyle='--', lw=2, color='r', label='Perfect model')\n\npyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('Recall')\npyplot.title('PR Curve')\npyplot.ylabel('Precision')\npyplot.legend()\n# show the plot\npyplot.show()\nplt.savefig('PR.jpg', transparent = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:08.704567Z","iopub.execute_input":"2023-03-15T00:12:08.705060Z","iopub.status.idle":"2023-03-15T00:12:09.273154Z","shell.execute_reply.started":"2023-03-15T00:12:08.705029Z","shell.execute_reply":"2023-03-15T00:12:09.271883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8),'axes.facecolor':'white', 'figure.facecolor':'white',\"figure.dpi\":100, 'savefig.dpi':300},font_scale = 2.5)\nsns.set_style(\"ticks\")\n\ny_pred_proba_log_test = ensemble.predict_proba(X_test)[::,1]\nfpr_log_test, tpr_log_test, _ = metrics.roc_curve(y_test,  y_pred_proba_log_test)\nauc_log_test = metrics.roc_auc_score(y_test, y_pred_proba_log_test)\n\ny_pred_proba_log_train = ensemble.predict_proba(X_train)[::,1]\nfpr_log_train, tpr_log_train, _ = metrics.roc_curve(y_train,  y_pred_proba_log_train)\nauc_log_train = metrics.roc_auc_score(y_train, y_pred_proba_log_train)\n\n#auc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr_log_test,tpr_log_test,label=\"Testing, AUC=\"+str(auc_log_test))\nplt.plot(fpr_log_train,tpr_log_train,label=\"Training, AUC=\"+str(auc_log_train))\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')\nplt.legend(loc=4)\n# axis labels\npyplot.title('ROC Curve')#\npyplot.ylabel('True positive rate')\npyplot.xlabel('False positive rate')\nplt.show()\nplt.savefig('ROC.jpg', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.275003Z","iopub.execute_input":"2023-03-15T00:12:09.275437Z","iopub.status.idle":"2023-03-15T00:12:09.867550Z","shell.execute_reply.started":"2023-03-15T00:12:09.275393Z","shell.execute_reply":"2023-03-15T00:12:09.866579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR = []\nLR.append('LR')\nLR.append(accuracy_score(y_test, y_pred) * 100)\nLR.append(precision_score(y_test, y_pred) * 100)\nLR.append(recall_score(y_test, y_pred) * 100)\nLR.append(f1_score(y_test, y_pred) * 100)\n#LR = pd.DataFrame([LR])\nLR","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.868749Z","iopub.execute_input":"2023-03-15T00:12:09.869061Z","iopub.status.idle":"2023-03-15T00:12:09.886741Z","shell.execute_reply.started":"2023-03-15T00:12:09.869032Z","shell.execute_reply":"2023-03-15T00:12:09.885581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = []\nKNN.append('KNN')\nKNN.append(accuracy_score(y_test, y_pred) * 100)\nKNN.append(precision_score(y_test, y_pred) * 100)\nKNN.append(recall_score(y_test, y_pred) * 100)\nKNN.append(f1_score(y_test, y_pred) * 100)\n#KNN = pd.DataFrame([KNN])\nKNN","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.887947Z","iopub.execute_input":"2023-03-15T00:12:09.888243Z","iopub.status.idle":"2023-03-15T00:12:09.900887Z","shell.execute_reply.started":"2023-03-15T00:12:09.888214Z","shell.execute_reply":"2023-03-15T00:12:09.899903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF = []\nRF.append('RF')\nRF.append(accuracy_score(y_test, y_pred) * 100)\nRF.append(precision_score(y_test, y_pred) * 100)\nRF.append(recall_score(y_test, y_pred) * 100)\nRF.append(f1_score(y_test, y_pred) * 100)\n#RF = pd.DataFrame([RF])\nRF","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.902051Z","iopub.execute_input":"2023-03-15T00:12:09.902345Z","iopub.status.idle":"2023-03-15T00:12:09.918572Z","shell.execute_reply.started":"2023-03-15T00:12:09.902318Z","shell.execute_reply":"2023-03-15T00:12:09.916783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBC = []\nGBC.append('GBC')\nGBC.append(accuracy_score(y_test, y_pred) * 100)\nGBC.append(precision_score(y_test, y_pred) * 100)\nGBC.append(recall_score(y_test, y_pred) * 100)\nGBC.append(f1_score(y_test, y_pred) * 100)\n#GBC = pd.DataFrame([GBC])\nGBC\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.921766Z","iopub.execute_input":"2023-03-15T00:12:09.922165Z","iopub.status.idle":"2023-03-15T00:12:09.939732Z","shell.execute_reply.started":"2023-03-15T00:12:09.922129Z","shell.execute_reply":"2023-03-15T00:12:09.938017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ensemble = []\nEnsemble.append('Ensemble')\nEnsemble.append(accuracy_score(y_test, y_pred) * 100)\nEnsemble.append(precision_score(y_test, y_pred) * 100)\nEnsemble.append(recall_score(y_test, y_pred) * 100)\nEnsemble.append(f1_score(y_test, y_pred) * 100)\n#Ensemble = pd.DataFrame([Ensemble])\nEnsemble","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.941514Z","iopub.execute_input":"2023-03-15T00:12:09.941939Z","iopub.status.idle":"2023-03-15T00:12:09.956246Z","shell.execute_reply.started":"2023-03-15T00:12:09.941904Z","shell.execute_reply":"2023-03-15T00:12:09.955245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = pd.concat([LR, KNN, RF, GBC,Ensemble], ignore_index = True)\ndf3","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:09.958091Z","iopub.execute_input":"2023-03-15T00:12:09.958476Z","iopub.status.idle":"2023-03-15T00:12:10.247832Z","shell.execute_reply.started":"2023-03-15T00:12:09.958445Z","shell.execute_reply":"2023-03-15T00:12:10.244681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata = pd.DataFrame(columns=[df3,'Name',\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\"]) ","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.249329Z","iopub.status.idle":"2023-03-15T00:12:10.250075Z","shell.execute_reply.started":"2023-03-15T00:12:10.249848Z","shell.execute_reply":"2023-03-15T00:12:10.249876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Plot multiple columns bar chart\ndata = [LR,KNN,RF,GBC]\ndf=pd.DataFrame(data,columns=['Model',\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\"])\n\ndf.plot(x=\"Model\", y=[\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\"], kind=\"bar\",figsize=(19,8))\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.252328Z","iopub.status.idle":"2023-03-15T00:12:10.253113Z","shell.execute_reply.started":"2023-03-15T00:12:10.252874Z","shell.execute_reply":"2023-03-15T00:12:10.252905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.255248Z","iopub.status.idle":"2023-03-15T00:12:10.255904Z","shell.execute_reply.started":"2023-03-15T00:12:10.255603Z","shell.execute_reply":"2023-03-15T00:12:10.255638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#power.fit(df_scaled_1) # values.reshape(-1,1) \npower.fit_transform(df_final_transformed)\nlbda = power.lambdas_\nthe_lambdas = pd.DataFrame((pd.DataFrame(lbda).T).values, columns = x.columns)\n\n#check  = ((df_scaled_1 + 1)**lbda - 1)/lbda # 0 <= y\ncheck = -((-df_scaled_1+1)**(2-lbda)-1)/(2-lbda) # y < 0\n#check  = ((2 + 1)**lbda - 1)/lbda\ncheck\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.257061Z","iopub.status.idle":"2023-03-15T00:12:10.257596Z","shell.execute_reply.started":"2023-03-15T00:12:10.257332Z","shell.execute_reply":"2023-03-15T00:12:10.257358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The lambda values\n\nthe_lamdas = pd.DataFrame((pd.DataFrame(lbda).T).values, columns = x.columns)\nthe_lamdas.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.258894Z","iopub.status.idle":"2023-03-15T00:12:10.259398Z","shell.execute_reply.started":"2023-03-15T00:12:10.259133Z","shell.execute_reply":"2023-03-15T00:12:10.259159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the_lamdas = # the_lamdas is the list above\n# sample = # the list of values of input features\n# model_input_features = []\n# for i in range(len(sample)):\n#     if sample[i] >= 0:\n#         y = ((sample[i] + 1)**lbda[i] - 1)/lbda[i]\n#     else:\n#         y = -((-sample[i] + 1)**(2-lbda[i])-1)/(2-lbda[i]))\n#     model_input_features.append(y)\n# model_input_features","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.260555Z","iopub.status.idle":"2023-03-15T00:12:10.261065Z","shell.execute_reply.started":"2023-03-15T00:12:10.260798Z","shell.execute_reply":"2023-03-15T00:12:10.260822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n#model = LogisticRegression()\nmodel.fit(df_scaled_1, y)\nimportances = pd.DataFrame(data={'Attribute': df_scaled.columns,'Importance': model.feature_importances_})\n#importances = pd.DataFrame(data={'Attribute': df_scaled.columns,'Importance': model.coef_[0]})\n\nimportances = importances.sort_values(by='Importance', ascending=False)\n\nplt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')\nplt.title('Feature importances obtained from coefficients', size=10)\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.262336Z","iopub.status.idle":"2023-03-15T00:12:10.262822Z","shell.execute_reply.started":"2023-03-15T00:12:10.262591Z","shell.execute_reply":"2023-03-15T00:12:10.262617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BoxPlot showing Processed distribution of features\n\nplt.figure(figsize=(13,10))\ndf_scaled.iloc[:,:].boxplot()\nplt.title('Percentile Distribution of Pre-processed Features', fontsize=17)\n# Get current axes\nax = plt.gca()\n# Make space for and rotate the x-axis tick labels\nplt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.264735Z","iopub.status.idle":"2023-03-15T00:12:10.265253Z","shell.execute_reply.started":"2023-03-15T00:12:10.265021Z","shell.execute_reply":"2023-03-15T00:12:10.265057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for SMOTE-ENN\n\nX = x_train\ny = y_train\n\ncounter = Counter(y)\nprint ('Before sampling:', counter)\nsmote_enn = SMOTEENN(random_state= 10)\nx_train, y_train = smote_enn.fit_resample(X,y)\ncounter = Counter(y_train)\nprint('After sampling:',counter)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.268395Z","iopub.status.idle":"2023-03-15T00:12:10.268916Z","shell.execute_reply.started":"2023-03-15T00:12:10.268700Z","shell.execute_reply":"2023-03-15T00:12:10.268726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Normal SMOTE\n\n#X = x_train\n#y = y_train\n#counter = Counter(y)\n#print(counter)\n#over = SMOTE(sampling_strategy=1.0)\n##under = RandomUnderSampler(sampling_strategy = 0.5)\n##steps = [('o', over),('u',under)]\n#steps = [('o', over)]\n#pipeline = Pipeline(steps = steps)\n#x_train, y_train = pipeline.fit_resample(X, y)\n## summarize the new class distribution\n#counter = Counter(y_train)\n#print(counter)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.270475Z","iopub.status.idle":"2023-03-15T00:12:10.271011Z","shell.execute_reply.started":"2023-03-15T00:12:10.270782Z","shell.execute_reply":"2023-03-15T00:12:10.270808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuning Hyperparameters based on Random Forest\n\nclf = RandomForestClassifier(n_jobs=-1) # {'min_samples_split': [3, 5, 10], 'n_estimators' : [100, 300],'max_depth': [3, 5, 15, 25],'max_features': [3, 5, 10, 20]}\n\nparam_grid = {'min_samples_split': [3, 5], 'n_estimators' : [100, 300],'max_depth': [15, 25],'max_features': [3, 5, 10]}\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\nscorers = {'precision_score': make_scorer(precision_score)}\nskf = StratifiedKFold(n_splits=10)\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nprec, recall, _ = precision_recall_curve(y_test, y_score, pos_label=clf.classes_[1])\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.272498Z","iopub.status.idle":"2023-03-15T00:12:10.272900Z","shell.execute_reply.started":"2023-03-15T00:12:10.272720Z","shell.execute_reply":"2023-03-15T00:12:10.272740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tuning Hyperparameters based on Logistic Regression\n\n# clf = LogisticRegressioln(n_jobs=-1) # \n# param_grid = {'c_values': [100, 10, 1.0, 0.1, 0.01], 'penalty' : ['l2'],'solvers': ['newton-cg', 'lbfgs', 'liblinear']}\n\n# #scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\n# scorers = {'precision_score': make_scorer(precision_score)}\n# skf = StratifiedKFold(n_splits=10)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.273794Z","iopub.status.idle":"2023-03-15T00:12:10.274124Z","shell.execute_reply.started":"2023-03-15T00:12:10.273970Z","shell.execute_reply":"2023-03-15T00:12:10.273986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tuning KNN hyperparameters based on Precision or Recall\n\n# clf = KNeighborsClassifier(n_jobs=-1) \n\n# param_grid = {'n_neighbors' : range(1, 5, 1),'weights' : ['uniform', 'distance'], 'metric' : ['euclidean', 'manhattan', 'minkowski', 'cosine']}\n# scorers = {'precision_score': make_scorer(precision_score),\n#            'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\n# skf = StratifiedKFold(n_splits=10)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.274921Z","iopub.status.idle":"2023-03-15T00:12:10.275261Z","shell.execute_reply.started":"2023-03-15T00:12:10.275096Z","shell.execute_reply":"2023-03-15T00:12:10.275114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tuning Hyperparameters based on Gradient Boost\n\n# clf = GradientBoostingClassifier(n_jobs=-1) # {'min_samples_split': [3, 5, 10], 'n_estimators' : [100, 300],'max_depth': [3, 5, 15, 25],'max_features': [3, 5, 10, 20]}\n\n# param_grid = {'n_estimators': [10, 100, 1000],'max_depth': [3, 9, 15],'learning_rate': [0.001, 0.01, 0.1],'subsample': [0.5, 0.7, 1.0]}\n\n# #scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\n# scorers = {'precision_score': make_scorer(precision_score)}\n# skf = StratifiedKFold(n_splits=10)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.276035Z","iopub.status.idle":"2023-03-15T00:12:10.276374Z","shell.execute_reply.started":"2023-03-15T00:12:10.276213Z","shell.execute_reply":"2023-03-15T00:12:10.276231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def grid_search_wrapper(refit_score='precision_score'):\n#     \"\"\"\n#     fits a GridSearchCV classifier using refit_score for optimization\n#     prints classifier performance metrics\n#     \"\"\"\n#     skf = StratifiedKFold(n_splits=10)\n#     grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n#     grid_search.fit(x_train.values, y_train.values)\n\n#     # make the predictions\n#     y_pred = grid_search.predict(x_test.values)\n\n#     print('Best params for {}'.format(refit_score))\n#     print(grid_search.best_params_)\n\n#     # confusion matrix on the test data.\n#     print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n#     print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n#                  columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n#     return grid_search\n\n# grid_search_clf = grid_search_wrapper(refit_score='precision_score')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.278421Z","iopub.status.idle":"2023-03-15T00:12:10.278850Z","shell.execute_reply.started":"2023-03-15T00:12:10.278642Z","shell.execute_reply":"2023-03-15T00:12:10.278662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []  #List to store the various model metrics \ndef models_lr(x,y):\n    mod = {}\n    model = LogisticRegression().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'LogisticRegression'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_lr(x_train,y_train))\n\ndef models_dt(x,y):\n    mod = {}\n    model = DecisionTreeClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Decision Tree'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_dt(x_train,y_train))\n\ndef models_rf(x,y):\n    mod = {}\n    model = RandomForestClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Random Forest'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_rf(x_train,y_train))\n\n#GradientBoostingClassifier\n\ndef models_gbc(x,y):\n    mod = {}\n    model = GradientBoostingClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Gradient Boost'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_gbc(x_train,y_train))\n\ndef models_nb(x,y):\n    mod = {}\n    model = GaussianNB().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'GaussianNB'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_nb(x_train,y_train))\n\ndef models_knn(x,y):\n    mod = {}\n    model = KNeighborsClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'KNN'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod   \nl.append(models_knn(x_train,y_train))\n\n#def models_ann(x,y):\n#    mod = {}\n#    model = Sequential()\n#    model.add(Dense(input_dim = 33, units = 10, activation='relu', kernel_initializer='he_uniform'))\n#    model.add(Dense(units = 12, activation='relu', kernel_initializer='uniform'))\n#    model.add(Dense(units = 20, activation='relu', kernel_initializer='uniform'))\n#    model.add(Dense(units = 1, activation='sigmoid', kernel_initializer='glorot_uniform'))\n#   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#   model.fit(x_train,y_train,batch_size=50, epochs=100)\n#    ypred = model.predict(x_test)\n#    mod['Model'] = 'ANN'\n#   mod['Train_Score'] = model.evaluate(x_train, y_train)\n#    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n#    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n#    mod['recall'] = metrics.recall_score(y_test, ypred)\n#    mod['precision'] = metrics.precision_score(y_test, ypred)\n#    model.predict_proba(x_test)\n#    mod['roc_auc'] = metrics.roc_aucroc_auc_score(y_test,ypred)\n#    return mod  \n#l.append(models_ann(x_train,y_train))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.279796Z","iopub.status.idle":"2023-03-15T00:12:10.280119Z","shell.execute_reply.started":"2023-03-15T00:12:10.279963Z","shell.execute_reply":"2023-03-15T00:12:10.279981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.282064Z","iopub.status.idle":"2023-03-15T00:12:10.282409Z","shell.execute_reply.started":"2023-03-15T00:12:10.282240Z","shell.execute_reply":"2023-03-15T00:12:10.282267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\nprint (tabulate(l))\n#l = pd.DataFrame(l,columns=[\"Model\",\"Train Accuracy\",\"Test Accuracy\",\"Test Recall\",\"Test Precision\",\"Test F1 Score\",\"ROC-AUC\"])\n#l","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.283385Z","iopub.status.idle":"2023-03-15T00:12:10.283745Z","shell.execute_reply.started":"2023-03-15T00:12:10.283569Z","shell.execute_reply":"2023-03-15T00:12:10.283588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Ranking using Recursive Feature Elimination\n\n-  Identify important features\n-  Remove irrelevant features\n-  Create new models based on the selected features","metadata":{}},{"cell_type":"code","source":"# explore the number of selected features for RFE\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.284917Z","iopub.status.idle":"2023-03-15T00:12:10.285236Z","shell.execute_reply.started":"2023-03-15T00:12:10.285074Z","shell.execute_reply":"2023-03-15T00:12:10.285092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For SMOTE-ENN\n\n#X = x_train\n#y = y_train\n#counter = Counter(y)\n#print ('Before sampling:', counter)\n#smote_enn = SMOTEENN(random_state= 10)\n#x_train, y_train = smote_enn.fit_resample(X,y)\n#counter = Counter(y_train)\n#print('After sampling:',counter)\n#X = x_train\n#y = y_train","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.286295Z","iopub.status.idle":"2023-03-15T00:12:10.286654Z","shell.execute_reply.started":"2023-03-15T00:12:10.286455Z","shell.execute_reply":"2023-03-15T00:12:10.286474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    for i in range(2, 10):\n        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n        model = DecisionTreeClassifier()\n        models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n    return models","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.287995Z","iopub.status.idle":"2023-03-15T00:12:10.288326Z","shell.execute_reply.started":"2023-03-15T00:12:10.288161Z","shell.execute_reply":"2023-03-15T00:12:10.288186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.289220Z","iopub.status.idle":"2023-03-15T00:12:10.289550Z","shell.execute_reply.started":"2023-03-15T00:12:10.289372Z","shell.execute_reply":"2023-03-15T00:12:10.289389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, x_train, y_train)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.290806Z","iopub.status.idle":"2023-03-15T00:12:10.291113Z","shell.execute_reply.started":"2023-03-15T00:12:10.290954Z","shell.execute_reply":"2023-03-15T00:12:10.290970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\n# fit RFE\nrfe_table = []\n#rfe_table = pd.DataFrame(columns=[\"Features\",\"Value\",\"Ranking\"])\nrfe.fit(x_train, y_train)\n# summarize all features\nfor i in range(X.shape[1]):\n    #print('Column: %d, Selected %s, Rank: %.3f' % (X.columns[i], rfe.support_[i], rfe.ranking_[i]))\n    print(' %s --- Selection: %s --- Rank: %.3f' % (X.columns[i], rfe.support_[i], rfe.ranking_[i]))\n    #rfe_table[[i,i,i]]= [[X.columns[i], rfe.support_[i], rfe.ranking_[i]]]\n    rfe_table.append([X.columns[i], rfe.support_[i], rfe.ranking_[i]])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.291852Z","iopub.status.idle":"2023-03-15T00:12:10.292135Z","shell.execute_reply.started":"2023-03-15T00:12:10.291992Z","shell.execute_reply":"2023-03-15T00:12:10.292007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model using top ranked-selected Features (For 10 input features)\n - Use Recursive feature elimination to identify important input features\n - Remove features that are not highly ranked\n - Build the models based on the features\n ","metadata":{}},{"cell_type":"code","source":"rfe_table = pd.DataFrame(rfe_table,columns=[\"Features\",\"Value\",\"Ranking\"])\nrfe_table","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.294449Z","iopub.status.idle":"2023-03-15T00:12:10.294914Z","shell.execute_reply.started":"2023-03-15T00:12:10.294735Z","shell.execute_reply":"2023-03-15T00:12:10.294756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy.iloc[5]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.296296Z","iopub.status.idle":"2023-03-15T00:12:10.296677Z","shell.execute_reply.started":"2023-03-15T00:12:10.296470Z","shell.execute_reply":"2023-03-15T00:12:10.296489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop some features based on the RFE rankings and Transform the dataset\nx = df_copy.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\ny_main = df_copy['Init_assess']\n\n# Keep only 10 selected input Features\nx = x.drop(['Smokes','IUD','Dx','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis',\n                  'STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n                  'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis',\n                  'STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n                  'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',\n\n# Transform dataset\nSS = StandardScaler()\nMMS = MinMaxScaler()\npower = PowerTransformer(method='yeo-johnson',standardize = False)# try 'yeo-johnson' or 'box-cox'\n\ndf_scaled = pd.DataFrame(power.fit_transform(x), columns = x.columns)\ndf_scaled = pd.DataFrame(SS.fit_transform(df_scaled), columns = df_scaled.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n#x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)\nx_train,x_test,y_train,y_test = train_test_split(df_scaled,y_main, stratify=y_main, test_size = 0.3, random_state = 121) # try 0.3, 0.35, 0.4\n\n\n\ndf_final_transformed = pd.DataFrame(SS.fit_transform(x), columns = x.columns)\ndf_final_model = pd.DataFrame(power.fit_transform(df_final_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n\n# x_train,x_test,y_train,y_test = train_test_split(x,y, stratify=y, test_size = 0.2, random_state = 212) # try 0.3, 0.35, 0.4\n# x_train_transformed = pd.DataFrame(SS.fit_transform(x_train), columns = x.columns)\n# x_train = pd.DataFrame(power.fit_transform(x_train_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n# x_test_transformed = pd.DataFrame(SS.fit_transform(x_test), columns = x.columns)\n# x_test = pd.DataFrame(power.fit_transform(x_test_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.297703Z","iopub.status.idle":"2023-03-15T00:12:10.298029Z","shell.execute_reply.started":"2023-03-15T00:12:10.297872Z","shell.execute_reply":"2023-03-15T00:12:10.297890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.299339Z","iopub.status.idle":"2023-03-15T00:12:10.299691Z","shell.execute_reply.started":"2023-03-15T00:12:10.299495Z","shell.execute_reply":"2023-03-15T00:12:10.299513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_test = x_test.drop(['Dx','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis','STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n#                   'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis','STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n#                   'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',\n\n# x_train = x_train.drop(['Dx','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis','STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n#                   'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis','STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n#                   'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',\n\n# x_test = x_test.drop(['Num of pregnancies','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis','STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n#                   'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis','STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n#                   'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',\n\n# x_train = x_train.drop(['Num of pregnancies','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis','STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n#                   'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis','STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n#                   'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.300563Z","iopub.status.idle":"2023-03-15T00:12:10.300879Z","shell.execute_reply.started":"2023-03-15T00:12:10.300720Z","shell.execute_reply":"2023-03-15T00:12:10.300741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train = x_train.drop([],axis=1)\n# y_train = y_train.drop([],axis=1)\ndf_final_model","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.302022Z","iopub.status.idle":"2023-03-15T00:12:10.302328Z","shell.execute_reply.started":"2023-03-15T00:12:10.302174Z","shell.execute_reply":"2023-03-15T00:12:10.302191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.303831Z","iopub.status.idle":"2023-03-15T00:12:10.304148Z","shell.execute_reply.started":"2023-03-15T00:12:10.303990Z","shell.execute_reply":"2023-03-15T00:12:10.304007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Normal SMOTE\n\nfrom imblearn.pipeline import Pipeline\nX = x_train\ny = y_train\ncounter = Counter(y)\nprint(counter)\nover = SMOTE(sampling_strategy=1.0)\n#under = RandomUnderSampler(sampling_strategy = 0.5)\n#steps = [('o', over),('u',under)]\nsteps = [('o', over)]\npipeline = Pipeline(steps = steps)\nx_train, y_train = pipeline.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y_train)\nprint(counter)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.305157Z","iopub.status.idle":"2023-03-15T00:12:10.305455Z","shell.execute_reply.started":"2023-03-15T00:12:10.305295Z","shell.execute_reply":"2023-03-15T00:12:10.305310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For SMOTE-ENN\n\n#X = x_train\n#y = y_train\n#counter = Counter(y)\n#print ('Before sampling:', counter)\n#smote_enn = SMOTEENN(random_state= 10)\n#x_train, y_train = smote_enn.fit_resample(X,y)\n#counter = Counter(y_train)\n#print('After sampling:',counter)\n#X = x_train\n#y = y_train","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.306628Z","iopub.status.idle":"2023-03-15T00:12:10.306924Z","shell.execute_reply.started":"2023-03-15T00:12:10.306773Z","shell.execute_reply":"2023-03-15T00:12:10.306790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\nmodel_list = []","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.307957Z","iopub.status.idle":"2023-03-15T00:12:10.308261Z","shell.execute_reply.started":"2023-03-15T00:12:10.308105Z","shell.execute_reply":"2023-03-15T00:12:10.308121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xtrain = x_train\n# xtest = x_test\n# ytrain = y_train\n# ytest = y_test\n\n# Tuning Hyperparameters based on KNN\nclf = KNeighborsClassifier(n_jobs=-1) \n\nparam_grid = {'n_neighbors' : range(1, 5, 1),'weights' : ['uniform', 'distance'], 'metric' : ['euclidean', 'manhattan', 'minkowski', 'cosine']}\nscorers = {'recall_score': make_scorer(recall_score)}\nskf = StratifiedKFold(n_splits=10)\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    model_Knn = grid_search.fit(x_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(x_test.values)\n    score = recall_score(y_test, y_pred)\n    scores.append(score)\n\n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n    print(score)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of KNN optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\nmodel_list.append(grid_search_clf)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.308939Z","iopub.status.idle":"2023-03-15T00:12:10.309258Z","shell.execute_reply.started":"2023-03-15T00:12:10.309107Z","shell.execute_reply":"2023-03-15T00:12:10.309122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.309947Z","iopub.status.idle":"2023-03-15T00:12:10.310229Z","shell.execute_reply.started":"2023-03-15T00:12:10.310086Z","shell.execute_reply":"2023-03-15T00:12:10.310102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load\ndump(grid_search_clf, 'filename.joblib') ","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.311021Z","iopub.status.idle":"2023-03-15T00:12:10.311301Z","shell.execute_reply.started":"2023-03-15T00:12:10.311156Z","shell.execute_reply":"2023-03-15T00:12:10.311171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuning Hyperparameters based on Logistic Regression\n\nclf = LogisticRegression(n_jobs=-1) # \nparam_grid = {'C': [1.1,1.0, 0.1, 0.01], 'penalty' : ['l2'],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\nscorers = {'recall_score': make_scorer(recall_score)}\nskf = StratifiedKFold(n_splits=10)\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    model_LR = grid_search.fit(x_train.values, y_train.values)\n    #model_LR = LogisticRegression(grid_search.best_params_)\n    #print(model_LR)\n\n    # make the predictions\n    y_pred = grid_search.predict(x_test.values)\n    score = recall_score(y_test, y_pred)\n    scores.append(score)\n    \n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of LR optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\nmodel_list.append(grid_search_clf)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.313133Z","iopub.status.idle":"2023-03-15T00:12:10.313451Z","shell.execute_reply.started":"2023-03-15T00:12:10.313286Z","shell.execute_reply":"2023-03-15T00:12:10.313305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuning Hyperparameters based on Random Forest\n\nclf = RandomForestClassifier(n_jobs=-1) # {'min_samples_split': [3, 5, 10], 'n_estimators' : [100, 300],'max_depth': [3, 5, 15, 25],'max_features': [3, 5, 10, 20]}\n\nparam_grid = {'min_samples_split': [3, 5], 'n_estimators' : [100, 110],'max_depth': [15, 20],'max_features': [3, 5, 7]}\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\nscorers = {'recall_score': make_scorer(recall_score)}\nskf = StratifiedKFold(n_splits=10)\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    grid_search.fit(x_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(x_test.values)\n    score = recall_score(y_test, y_pred)\n    scores.append(score)\n    \n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\nmodel_list.append(grid_search_clf)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.314300Z","iopub.status.idle":"2023-03-15T00:12:10.314638Z","shell.execute_reply.started":"2023-03-15T00:12:10.314447Z","shell.execute_reply":"2023-03-15T00:12:10.314463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Tuning Hyperparameters based on Gradient Boost\n\nclf = GradientBoostingClassifier() # {'min_samples_split': [3, 5, 10], 'n_estimators' : [100, 300],'max_depth': [3, 5, 15, 25],'max_features': [3, 5, 10, 20]}\n\nparam_grid = {'n_estimators': [10, 100],'max_depth': [3, 9, 12],'learning_rate': [0.001, 0.01, 0.1],'subsample': [0.5, 0.7, 1.0]}\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\nscorers = {'recall_score': make_scorer(recall_score)}\n\nskf = StratifiedKFold(n_splits=10)\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    grid_search.fit(x_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(x_test.values)\n    score = recall_score(y_test, y_pred)\n    scores.append(score)\n    \n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of GBC optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\nmodel_list.append(grid_search_clf)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.315496Z","iopub.status.idle":"2023-03-15T00:12:10.315813Z","shell.execute_reply.started":"2023-03-15T00:12:10.315664Z","shell.execute_reply":"2023-03-15T00:12:10.315681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.317733Z","iopub.status.idle":"2023-03-15T00:12:10.318249Z","shell.execute_reply.started":"2023-03-15T00:12:10.317998Z","shell.execute_reply":"2023-03-15T00:12:10.318021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_list = list()\n# model_list.append(('KNN', KNeighborsClassifier(metric= 'cosine', n_neighbors = 4, weights = 'distance').fit(x_train, y_train)))\n# model_list.append(('LR', LogisticRegression(C =  1.1, penalty = 'l2', solver = 'newton-cg').fit(x_train, y_train)))\n# model_list.append(('RF', RandomForestClassifier(max_depth = 15, max_features = 5, min_samples_split = 3, n_estimators = 100).fit(x_train, y_train)))\n# model_list.append(('GBC', GradientBoostingClassifier(learning_rate= 0.1, max_depth= 9, n_estimators= 100, subsample =  0.7).fit(x_train, y_train)))\n# GradientBoostingClassifier()\n\n# weight = []\n# best_score = []\n# samples = []\n# score = 0\n# scorelist =[]\n# results = []\n# for w in np.arange(0.1,1.2,.4): # Knn\n#     for x in np.arange(0.1,1.2,.4): # LR\n#         for y in np.arange(0.1,1.2,.4): # RF\n#             for z in np.arange(0.1,1.2,.4): # GBC\n#                 weight = [w,x,y,z]\n#                 ensemble = VotingClassifier(estimators = model_list, voting='soft', weights=weight) # weights=scores\n#                 ensemble.fit(x_train, y_train)\n#                 # make predictions on test set\n#                 y_pred = ensemble.predict(x_test)\n#                 # evaluate predictions\n#                 score = recall_score(y_test, y_pred)\n#                 results.append(weight)\n#                 scorelist.append(score)\n#                 #print('Weighted Avg Recall: %.3f' % (score*100))\n\n# # confusion matrix on the test data.\n# #print('\\nConfusion matrix of GBC optimized for {} on the test data:'.format(refit_score))\n# print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n#              columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n\n# Knn = KNeighborsClassifier(metric= 'cosine', n_neighbors = 4, weights = 'distance').fit(x_train, y_train)\n# LR = LogisticRegression(C =  1.1, penalty = 'l2', solver = 'newton-cg').fit(x_train, y_train)\n# RF = RandomForestClassifier(max_depth = 15, max_features = 5, min_samples_split = 3, n_estimators = 100).fit(x_train, y_train)\n# GBC = GradientBoostingClassifier(learning_rate= 0.1, max_depth= 9, n_estimators= 100, subsample =  0.7).fit(x_train, y_train)\n# filename = 'finalized_model_Knn.sav'\n# joblib.dump(Knn, filename)\n# filename = 'finalized_model_LR.sav'\n# joblib.dump(LR, filename)\n# filename = 'finalized_model_RF.sav'\n# joblib.dump(RF, filename)\n# filename = 'finalized_model_GBC.sav'\n# joblib.dump(GBC, filename)\n# filename = 'finalized_model_ensemble.sav'\n# joblib.dump(ensemble, filename)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.319296Z","iopub.status.idle":"2023-03-15T00:12:10.319655Z","shell.execute_reply.started":"2023-03-15T00:12:10.319474Z","shell.execute_reply":"2023-03-15T00:12:10.319491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# sc = np.array(scorelist)\n\n# sc = sc.astype(np.float)\n# print('Maximun recall score is:',sc.max()*100)\n# sc.max()\n# #np.where(sc=0.3684210526315789)\n# #max_index = sc.index(sc.max())\n# #print('With the weight of KNN, LR, RF, GBC respectively:',max_index)\n# scorelist,results\n# #print (tabulate(sc))\n# #results\n# wt = results\n# sc = pd.DataFrame(scorelist, columns=['Recall_score'])\n# results = pd.DataFrame(results, columns=['KNN','LR','RF','GBC'])\n# sure = pd.concat([results,sc], axis = 1)\n# sure\n# sure.to_csv('weight_result2.csv')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.321038Z","iopub.status.idle":"2023-03-15T00:12:10.321371Z","shell.execute_reply.started":"2023-03-15T00:12:10.321210Z","shell.execute_reply":"2023-03-15T00:12:10.321228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x in np.arange(0,1.2,.2):\n#     print (x)\ny","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.322564Z","iopub.status.idle":"2023-03-15T00:12:10.322880Z","shell.execute_reply.started":"2023-03-15T00:12:10.322726Z","shell.execute_reply":"2023-03-15T00:12:10.322743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Normal SMOTE\n\nfrom imblearn.pipeline import Pipeline\nX = df_final_model\ny = y_main\ncounter = Counter(y)\nprint(counter)\nover = SMOTE(sampling_strategy=1.0)\n#under = RandomUnderSampler(sampling_strategy = 0.5)\n#steps = [('o', over),('u',under)]\nsteps = [('o', over)]\npipeline = Pipeline(steps = steps)\nx_train, y_train = pipeline.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y_train)\nprint(counter)\n\n# scores = []\n# model_list = []\n\nmodel_list = list()\nmodel_list.append(('KNN', KNeighborsClassifier(metric= 'cosine', n_neighbors = 2, weights = 'distance').fit(x_train, y_train)))\nmodel_list.append(('LR', LogisticRegression(C =  1.1, penalty = 'l2', solver = 'newton-cg').fit(x_train, y_train)))\nmodel_list.append(('RF', RandomForestClassifier(max_depth = 15, max_features = 5, min_samples_split = 3, n_estimators = 100).fit(x_train, y_train)))\nmodel_list.append(('GBC', GradientBoostingClassifier(learning_rate= 0.1, max_depth= 9, n_estimators= 100, subsample =  0.7).fit(x_train, y_train)))\nGradientBoostingClassifier()\n\nensemble = VotingClassifier(estimators = model_list, voting='soft', weights=scores) # weights=scores\nensemble.fit(x_train, y_train) # x_train, y_train   df_final_model,y\n# make predictions on test set\ny_pred = ensemble.predict(x_test)\n# evaluate predictions\nscore = recall_score(y_test, y_pred)\nprint('Weighted Avg Recall: %.3f' % (score*100))\n\n# confusion matrix on the test data.\n#print('\\nConfusion matrix of GBC optimized for {} on the test data:'.format(refit_score))\nprint(pd.DataFrame(confusion_matrix(y_test, y_pred),\n             columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n\nKnn = KNeighborsClassifier(metric= 'cosine', n_neighbors = 2, weights = 'distance').fit(df_final_model,y)#(x_train, y_train)\nLR = LogisticRegression(C =  1.1, penalty = 'l2', solver = 'newton-cg').fit(df_final_model,y)#(x_train, y_train)\nRF = RandomForestClassifier(max_depth = 15, max_features = 5, min_samples_split = 3, n_estimators = 100).fit(df_final_model,y)#(x_train, y_train)\nGBC = GradientBoostingClassifier(learning_rate= 0.1, max_depth= 9, n_estimators= 100, subsample =  0.7).fit(df_final_model,y)#(x_train, y_train)\nfilename = 'finalized_model_Knn.sav'\njoblib.dump(Knn, filename)\nfilename = 'finalized_model_LR.sav'\njoblib.dump(LR, filename)\nfilename = 'finalized_model_RF.sav'\njoblib.dump(RF, filename)\nfilename = 'finalized_model_GBC.sav'\njoblib.dump(GBC, filename)\nfilename = 'finalized_model_ensemble.sav'\njoblib.dump(ensemble, filename)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.324164Z","iopub.status.idle":"2023-03-15T00:12:10.324507Z","shell.execute_reply.started":"2023-03-15T00:12:10.324333Z","shell.execute_reply":"2023-03-15T00:12:10.324357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.326130Z","iopub.status.idle":"2023-03-15T00:12:10.326501Z","shell.execute_reply.started":"2023-03-15T00:12:10.326300Z","shell.execute_reply":"2023-03-15T00:12:10.326318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finalized_model.sav","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.327579Z","iopub.status.idle":"2023-03-15T00:12:10.327900Z","shell.execute_reply.started":"2023-03-15T00:12:10.327743Z","shell.execute_reply":"2023-03-15T00:12:10.327761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []  #List to store the various model metrics \ndef models_lr(x,y):\n    mod = {}\n    model = LogisticRegression().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'LogisticRegression'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_lr(x_train,y_train))\n\ndef models_dt(x,y):\n    mod = {}\n    model = DecisionTreeClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Decision Tree'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_dt(x_train,y_train))\n\ndef models_rf(x,y):\n    mod = {}\n    model = RandomForestClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Random Forest'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_rf(x_train,y_train))\n\n#GradientBoostingClassifier\n\ndef models_gbc(x,y):\n    mod = {}\n    model = GradientBoostingClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Gradient Boost'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_gbc(x_train,y_train))\n\ndef models_nb(x,y):\n    mod = {}\n    model = GaussianNB().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'GaussianNB'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod\nl.append(models_nb(x_train,y_train))\n\ndef models_knn(x,y):\n    mod = {}\n    model = KNeighborsClassifier().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'KNN'\n    mod['Train_Score'] = model.score(x_train,y_train)\n    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n    mod['recall'] = metrics.recall_score(y_test, ypred)\n    mod['precision'] = metrics.precision_score(y_test, ypred)\n    #mod['proba'] = model.predict_proba(x_test)\n    mod['F1-score'] = metrics.f1_score(y_test,ypred)\n    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n    mod['cm'] = confusion_matrix(y_test,ypred)\n    return mod   \nl.append(models_knn(x_train,y_train))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.329009Z","iopub.status.idle":"2023-03-15T00:12:10.329362Z","shell.execute_reply.started":"2023-03-15T00:12:10.329180Z","shell.execute_reply":"2023-03-15T00:12:10.329197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l\nfrom tabulate import tabulate\n\n#headers = [\"Model\", \"Train Score\", \"Test Score\",\"F1-Score\",\"recall\",\"precision\",\"roc_auc\"]\n#print(tabulate(l,headers = headers, tablefmt = \"grid\"))\nprint (tabulate(l))\nresult = pd.DataFrame(l)\nresult.shape\nresult","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.330828Z","iopub.status.idle":"2023-03-15T00:12:10.331411Z","shell.execute_reply.started":"2023-03-15T00:12:10.331146Z","shell.execute_reply":"2023-03-15T00:12:10.331175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ensemble# Create one model for all targets\n\n- select two positive observations for Biopsy, Citology, Hinselmann, Schiller\n- Test all the conditions for all the feautre\n\n# For Weighted-Average-Model Anaysis\n\n- we will define a function to create a list of weighted models to use in the ensemble.\n\n","metadata":{}},{"cell_type":"code","source":"df_copy","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.333451Z","iopub.status.idle":"2023-03-15T00:12:10.333877Z","shell.execute_reply.started":"2023-03-15T00:12:10.333693Z","shell.execute_reply":"2023-03-15T00:12:10.333713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = df_copy[['Biopsy','Hinselmann','Citology','Schiller','Init_assess']]\nx = df_copy.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\n#y = df_copy['Schiller']\n\n\n# Keep only 10 selected input Features\nx = x.drop(['Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis',\n                  'STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n                  'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis',\n                  'STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n                  'STDs:condylomatosis','STDs','IUD','Smokes (packs/year)','Smokes (years)'],axis=1)\n\n# Transform dataset\nSS = StandardScaler()\nMMS = MinMaxScaler()\npower = PowerTransformer(method='yeo-johnson')# try 'yeo-johnson' or 'box-cox'\ndf_scaled = pd.DataFrame(power.fit_transform(x), columns = x.columns)\ndf_scaled = pd.DataFrame(SS.fit_transform(df_scaled), columns = df_scaled.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n\ndata = pd.concat([df_scaled,target], axis=1)\ntrain = pd.concat([df_scaled,target], axis=1)\n\n#B = [47,31]\n#C = [71,18]\n#H = [269,258]\n#S = [32,44]\nN = [132,133,227,255,70,18,269,258,32,44,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,134,135]\ntrain.drop(train.index[N], inplace = True)\n#data.to_csv(\"data_for_training.csv\")\n#wer = data.drop([data.index[2]])\n#wer = data.index[13]\ntrain.shape\n\ntest = pd.concat([data,train]).drop_duplicates(keep=False)\ntest.reset_index(drop = True, inplace = True)\ntest","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.334903Z","iopub.status.idle":"2023-03-15T00:12:10.335253Z","shell.execute_reply.started":"2023-03-15T00:12:10.335087Z","shell.execute_reply":"2023-03-15T00:12:10.335107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.336840Z","iopub.status.idle":"2023-03-15T00:12:10.337204Z","shell.execute_reply.started":"2023-03-15T00:12:10.337042Z","shell.execute_reply":"2023-03-15T00:12:10.337061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nm = []#\n#l = pd.DataFrame(l,columns=[\"Biopsy\",\"Cytology\",\"Hinselmann\",\"Schiller\"])\n#l = pd.DataFrame(columns=[\"Biopsy\",\"Cytology\",\"Hinselmann\",\"Schiller\"])\nfor label in range(5):#\n    #which = l.columns[label]\n    l = []\n    for samples in range(len(test.index)):\n        x_train = train.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\n        types = ['Biopsy','Hinselmann','Citology','Schiller','Init_assess']\n        target = types[label]\n        y_train = train[target]\n        x_test = test.iloc[[samples]]\n        x_test = x_test.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\n        y_test = test.iloc[[samples]][target]\n        #print(y_test)\n        target = types[label]\n        X = x_train\n        y = y_train\n        counter = Counter(y)\n        #print ('Before sampling:', counter)\n        smote_enn = SMOTEENN(random_state= 10)\n        x_train, y_train = smote_enn.fit_resample(X,y)\n        counter = Counter(y_train)\n        #print('After sampling:',counter)\n       # l = []  #List to store the various model metrics \n        def models_lr(x,y):\n            #mod = {}#\n            mod = []\n            model = KNeighborsClassifier().fit(x,y)\n            #model = LogisticRegression().fit(x,y)\n            #model = RandomForestClassifier().fit(x,y)\n            #model = DecisionTreeClassifier().fit(x,y)\n            #model = GradientBoostingClassifier().fit(x,y)\n            model_log = LogisticRegression().fit(x,y)#\n            ypred = model.predict(x_test)#\n            #mod['prediction'] = ypred#\n            pred = int(ypred)#\n            #mod['Model'] = 'Logistic Regression'#\n            #mod['Train_Score'] = model.score(x_train,y_train)#\n            #mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)#\n            #mod['recall'] = metrics.recall_score(y_test, ypred)#\n            #mod['precision'] = metrics.precision_score(y_test, ypred)#\n            #mod['proba'] = model.predict_proba(x_test)#\n            proba = model_log.predict_proba(x_test)\n            proba = np.around(proba, decimals = 3)\n            #mod['F1-score'] = metrics.f1_score(y_test,ypred)#\n            #mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)#\n            #mod['cm'] = confusion_matrix(y_test,ypred)#\n            #mod = pred,proba#\n            mod = pred\n            return mod#\n        #l[which,samples] = models_lr(x_train,y_train)\n        #l[which] = [models_lr(x_train,y_train),which]\n        #l[which] = [which,models_lr(x_train,y_train)]\n        #l[which].append([which,models_lr(x_train,y_train)])\n        l.append(models_lr(x_train,y_train))#\n    m.append(l)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.338466Z","iopub.status.idle":"2023-03-15T00:12:10.338811Z","shell.execute_reply.started":"2023-03-15T00:12:10.338657Z","shell.execute_reply":"2023-03-15T00:12:10.338674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#l = pd.DataFrame(l,columns=['logOdds',\"value\"])\nl = pd.DataFrame(l)\n\nm = pd.DataFrame(m)\nm = m.transpose()\nm.columns=[\"Biopsy\",\"Citology\",\"Hinselmann\",\"schiller\",'Init_assess']\nm","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.339590Z","iopub.status.idle":"2023-03-15T00:12:10.339881Z","shell.execute_reply.started":"2023-03-15T00:12:10.339733Z","shell.execute_reply":"2023-03-15T00:12:10.339749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a list of base models\ndef get_models():\n    models = list()\n    models.append(('lr', LogisticRegression()))\n    models.append(('DT', DecisionTreeClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('bayes', GaussianNB()))\n    return models\n \n# evaluate each base model\ndef evaluate_models(models, X_train, X_val, y_train, y_val):\n    # fit and evaluate the models\n    scores = list()\n    for name, model in models:\n        # fit the model\n        model.fit(X_train, Y_train)\n        # evaluate the model\n        yhat = model.predict(X_val)\n        acc = accuracy_score(y_val, yhat)\n        # store the performance\n        scores.append(acc)\n        # report model performance\n    return scores\n \n# define dataset\n##X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n# split dataset into train and test sets\n#X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n# split the full train set into train and validation sets\nX_train, X_val, Y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n# create the base models\nmodels = get_models()\n# fit and evaluate each model\nscores = evaluate_models(models, X_train, X_val, Y_train, y_val)\nprint(scores)\n# create the ensemble\nensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n# fit the ensemble on the training dataset\nensemble.fit(x_train, y_train)\n# make predictions on test set\nyhat = ensemble.predict(x_test)\n# evaluate predictions\nscore = accuracy_score(y_test, yhat)\nprint('Weighted Avg Accuracy: %.3f' % (score*100))\n# evaluate each standalone model\nscores = evaluate_models(models, x_train, x_test, y_train, y_test)\nfor i in range(len(models)):\n    print('>%s: %.3f' % (models[i][0], scores[i]*100))\n# evaluate equal weighting\nensemble = VotingClassifier(estimators=models, voting='soft')\nensemble.fit(x_train, y_train)\nyhat = ensemble.predict(x_test)\nscore = accuracy_score(y_test, yhat)\nprint('Voting Accuracy: %.3f' % (score*100))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.340729Z","iopub.status.idle":"2023-03-15T00:12:10.341015Z","shell.execute_reply.started":"2023-03-15T00:12:10.340868Z","shell.execute_reply":"2023-03-15T00:12:10.340884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Drop some features based on the RFE rankings and Transform the dataset\nx = df_copy.drop(['Biopsy','Hinselmann','Citology','Schiller','Init_assess'],axis=1)\ny_train = df_copy['Init_assess']\n\n# Keep only 10 selected input Features\nx = x.drop(['Dx','Dx:HPV','Dx:CIN','Dx:Cancer','STDs: Number of diagnosis',\n                  'STDs:HPV','STDs:Hepatitis B','STDs:HIV','STDs:molluscum contagiosum',\n                  'STDs:genital herpes','STDs:pelvic inflammatory disease','STDs:syphilis',\n                  'STDs:vulvo-perineal condylomatosis','STDs:vaginal condylomatosis',\n                  'STDs:condylomatosis','STDs'],axis=1) # ,'IUD','Smokes',\n\n\n# Transform dataset\nSS = StandardScaler()\nMMS = MinMaxScaler()\npower = PowerTransformer(method='yeo-johnson',standardize = False)# try 'yeo-johnson' or 'box-cox'\n\nx_train_transformed = pd.DataFrame(SS.fit_transform(x), columns = x.columns)\nx_train = pd.DataFrame(power.fit_transform(x_train_transformed), columns = x.columns) # try Minmax scaler 'MMS' or Standard Scaler 'SS'\n\n\n# For Normal SMOTE\n\nfrom imblearn.pipeline import Pipeline\nX = x_train\ny = y_train\ncounter = Counter(y)\nprint(counter)\nover = SMOTE(sampling_strategy=1.0)\n#under = RandomUnderSampler(sampling_strategy = 0.5)\n#steps = [('o', over),('u',under)]\nsteps = [('o', over)]\npipeline = Pipeline(steps = steps)\nx_train, y_train = pipeline.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y_train)\nprint(counter)\n\n\n# Train and tune a KNN model with its Hyperparameters based on KNN\nclf = KNeighborsClassifier(n_jobs=-1) \n\nparam_grid = {'n_neighbors' : range(1, 5, 1),'weights' : ['uniform', 'distance'], 'metric' : ['euclidean', 'manhattan', 'minkowski', 'cosine']}\nscorers = {'recall_score': make_scorer(recall_score)}\nskf = StratifiedKFold(n_splits=10)\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    model_Knn = grid_search.fit(x_train.values, y_train.values)\n\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\n#model_list.append(grid_search_clf)\n\n\n# Tuning Hyperparameters based on Gradient Boost\n\nclf = GradientBoostingClassifier() # {'min_samples_split': [3, 5, 10], 'n_estimators' : [100, 300],'max_depth': [3, 5, 15, 25],'max_features': [3, 5, 10, 20]}\n\nparam_grid = {'n_estimators': [10, 100],'max_depth': [3, 9, 12],'learning_rate': [0.001, 0.01, 0.1],'subsample': [0.5, 0.7, 1.0]}\n\n#scorers = {'precision_score': make_scorer(precision_score),'recall_score': make_scorer(recall_score),'accuracy_score': make_scorer(accuracy_score)}\nscorers = {'recall_score': make_scorer(recall_score)}\n\nskf = StratifiedKFold(n_splits=10)\n\n# Build the model on the tuned model\ndef grid_search_wrapper(refit_score='recall_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=True, n_jobs=-1)\n    grid_search.fit(x_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(x_test.values)\n    score = recall_score(y_test, y_pred)\n    scores.append(score)\n    \n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of GBC optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n\ngrid_search_clf = grid_search_wrapper(refit_score='recall_score')\n#model_list.append(grid_search_clf)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T00:12:10.341877Z","iopub.status.idle":"2023-03-15T00:12:10.342158Z","shell.execute_reply.started":"2023-03-15T00:12:10.342015Z","shell.execute_reply":"2023-03-15T00:12:10.342031Z"},"trusted":true},"execution_count":null,"outputs":[]}]}